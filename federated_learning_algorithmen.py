# -*- coding: utf-8 -*-
"""Federated Learning Algorithmen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WWMaj0Quo9mTjUjlgZbCLsCKlfLBbV-B
"""

print(tf.__version__)

"""## Imports (als erstes ausführen!)"""

import numpy as np
import random
import cv2
import os
from imutils import paths
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.optimizers import SGD
from tensorflow.keras import backend as K

import tensorflow_datasets as tfds
import itertools
import time

import matplotlib.pyplot as plt

"""## Funktionen (als zweites ausführen!)"""

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

idx = np.argsort(y_train)
x_train_sorted = x_train[idx]
y_train_sorted = y_train[idx]

def create_clients_iid(x_train_sorted, y_train_sorted, num_clients=10):
  '''
  return: Gibt ein Dictionary zurück, in der pro Client die Daten in data shards gespeichert werden. Die Daten sind iid.
  args: x_train_sorted: nach Label-Klasse sortierte Liste von numpy arrays der Trainingsdaten
        y_train_sorted: Liste der zu x_train_sorted gehörigen Labels (entsprechend auch sortiert)
        num_clients: Anzahl an Clients im Federated Network
  '''
  #Ertellt ein Dictionary von Clients, wobei Anzahl=num_clients
  client_names = ['clients_{}'.format(i+1) for i in range(num_clients)]

  # Bestimmt jeweils die erste Position einer Label-Klasse (Hinweis: die Daten sind sortiert)
  number_begin_index = []
  for i in range(10):
    number_begin_index.append(np.where(y_train_sorted==i)[0][0])

  # Die Listen werden verlinkt
  data_combined = list(zip(x_train_sorted, y_train_sorted))


  #Bestimmt die Anzahl an Daten, die jeder Client pro Label-Klasse erhält (5421 ist die Anzahl an '5' in MNIST) 
  size = (5421)//num_clients
  shards = 10*[0] 
  for label in range(10):
      shards[label] =  [data_combined[int(number_begin_index[label]+ size*(client_index) ):int(number_begin_index[label]+size*(client_index+1))] for client_index in range(num_clients)] 


  return {client_names[client_index]: [shards[label][client_index] for label in range(10)] for client_index in range(num_clients) }



def create_clients_non_iid(x_train_sorted, y_train_sorted, num_clients=10, dirichlet_param=0.3):
  '''
  return: Gibt ein Dictionary zurück, in der pro Client die Daten in data shards gespeichert werden. Die Daten sind non-iid.
  args: x_train_sorted: nach Label-Klasse sortierte Liste von numpy arrays der Trainingsdaten
        y_train_sorted: Liste der zu x_train_sorted gehörigen Labels (entsprechend auch sortiert)
        num_clients: Anzahl an Clients im Federated Network
        dirichlet_param: gibt den Parameter>0 der Dirichlet-Verteilung an, mit der die Verteilung der Label-Klassen bestimmt wird. Je größer dirichlet_param, desto näher an iid
  '''
  #Ertellt ein Dictionary von Clients, wobei Anzahl=num_clients
  client_names = ['clients_{}'.format(i+1) for i in range(num_clients)]

  # Bestimmt jeweils die erste Position einer Label-Klasse (Hinweis: die Daten sind sortiert)
  number_begin_index = []
  for i in range(10):
    number_begin_index.append(np.where(y_train_sorted==i)[0][0])

  # Die Listen werden verlinkt
  data_combined = list(zip(x_train_sorted, y_train_sorted))

  # Hier wird geprüft, ob durch die Clients jede Label-Klasse (global gesehen) häufig genug vorkommen wird, wenn nein wird die Verteilung neu generiert.
  #Damit soll ermöglicht werden, dass die Menge an genutzten Trainingsdaten hoch gehalten wird
  t = True
  while t:
    data_ratios = np.random.dirichlet(dirichlet_param*np.ones(10), num_clients).transpose()
    max_number = max([sum(data_ratios[i]) for i in range(10)]) / num_clients
    min_number = min([sum(data_ratios[i]) for i in range(10)]) / num_clients
    if max_number<0.12 and min_number>0.06:
      t = False

  #Bestimmt einen Skalierungsfaktor, damit der Client die Korrekte Anzahl an Label-Klassen erhält und möglichst viele Label-Klassen genutzt werden 
  scale_size = ((5421*10)*0.1)//(num_clients*max_number) 
  data_ratios_scaled = (data_ratios*scale_size).round() 


  #Bestimmt die Anzahl an Daten, die jeder Client pro Label-Klasse erhält (5421 ist die Anzahl an '5' in MNIST) 
  shards = 10*[0] 
  for label in range(10):
      shards[label] =  [data_combined[int(number_begin_index[label]+sum(data_ratios_scaled[label][0:client_index])):int(number_begin_index[label]+sum(data_ratios_scaled[label][0:client_index+1]))] for client_index in range(num_clients)] 


  return {client_names[client_index]: [shards[label][client_index] for label in range(10)] for client_index in range(num_clients) }

def get_clients_batched(clients,B_size):
  #Die bisher noch sortierten Daten im Dictionary werden in dieser Funktion in ein neues Dictionary bespeichert, wobei die Daten diesmal gemischt und in Batches der Größe B_size aufgeteilt werden
  clients_batched = dict()
  for(client_name,data) in clients.items():
    new_data = []
    for label in range(10):
      new_data.extend(data[label])
    clients_batched[client_name] =  batch_data(new_data,B_size)
  return clients_batched

def get_clients_unbatched(clients):
  #Die bisher noch sortierten Daten im Dictionary werden in dieser Funktion in ein neues Dictionary bespeichert, wobei die Daten diesmal gemischt werden (Ohne Aufteilung in Batches)
  clients_batched = dict()
  for(client_name,data) in clients.items():
    new_data = []
    for label in range(10):
      new_data.extend(data[label])
    clients_batched[client_name] =  unbatch_data(new_data)
  return clients_batched



def unbatch_data(data_shard):
  '''
  Takes in a clients data shard and create a tfds object off it
  args: shards: a data, label constituting a client's data shard
  return:
        tfds object tensorflow dataset
  '''
  #seperate shard into data and label lists
  data, label = zip(*data_shard)
  dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))
  return dataset.shuffle(len(label)).batch(len(data))
  
def batch_data(data_shard, bs=32):
  '''
  Takes in a clients data shard and create a tfds object off it
  args: shards: a data, label constituting a client's data shard
        bs: batch size
  return:
        tfds object tensorflow dataset
  '''
  #seperate shard into data and label lists
  data, label = zip(*data_shard)
  dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))
  return dataset.shuffle(len(label)).batch(bs)

def weight_scalling_factor(clients_dict, active_clients, client_name):
  '''  '''
  #Ermittle Batch Size bs
  bs = list(clients_dict[client_name])[0][0].shape[0]
  #Anzahl an Daten aller aktiven Clients
  global_count = sum([tf.data.experimental.cardinality(clients_dict[client_name]).numpy() for client_name in active_clients])*bs
  #Anzahl an Daten aller aktiven Clients
  local_count = tf.data.experimental.cardinality(clients_dict[client_name]).numpy()*bs
  return local_count/global_count

def scale_model_weights(weight, scalar):
  ''' fuction for scaling a models weights'''
  weight_final = []
  steps = len(weight)
  for i in range(steps):
    weight_final.append(scalar*weight[i])
  return weight_final

def sum_scaled_weights(scaled_weight_list):
  ''' Returns the sum of the listed scaled weights. This is quivalent to scaled avg of the weights'''
  avg_grad = []
  # Berechnet den 
  for grad_list_tuple in zip(*scaled_weight_list):
    layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
    avg_grad.append(layer_mean)
  return avg_grad


def get_gradients(model : tf.keras.Model, x, y_true):
    """Return the gradient of every trainable weight in model"""
    if type(y_true) is np.ndarray:
      y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)
    if type(x) is np.ndarray:
      x = tf.convert_to_tensor(x, dtype=tf.float32)
   
    with tf.GradientTape() as tape:
        loss = model.compiled_loss(y_true, model(x))
    return tape.gradient(loss, model.trainable_weights)


def test_model(X_test,Y_test, model, comm_round):
  ''' Diese Funktion berechnet den Testfehler des globalen Modells anhand der Testdaten X_test,Y_test
      Außerdem werden nach jedem Test die Modell-Genauigkeit und der Modell-Fehler ausgegeben '''
  cce = tf.keras.losses.SparseCategoricalCrossentropy()
  logits = model.predict(X_test)
  loss = cce(Y_test, logits)
  lb = LabelBinarizer()
  Y_test = lb.fit_transform(Y_test)
  acc = accuracy_score(tf.argmax(logits,axis=1), tf.argmax(Y_test,axis=1))
  print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
  return acc,loss

class SimpleMLP:
  ''' Hier wird das ML-Modell der Clients und des zentralen Servers erstellt '''
  @staticmethod
  def build(shape, classes):
    model = tf.keras.models.Sequential([
      tf.keras.layers.Flatten(input_shape=(shape, shape)),
      tf.keras.layers.Dense(200, activation='relu'),
      tf.keras.layers.Dense(200, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
    ])
    return model

"""## FL-Algortihmen (als drittes ausführen!)"""

def FedAvg(clients_batched,active_client_ratio, learning_rate, comms_round=10, E_size=1):
  '''
  Dieser Algorithmus orientiert sich an Algorithmus 1 aus:
  B. McMahan, E. Moore, D. Ramage, S. Hampson, B. A. Arcas. “Communication-efficient learning of deep networks from decentralized data”. In: Artificial Intelligence and Statistics. PMLR. 2017, pp. 1273–1282. arXiv:1602.05629
  --------------------------------------------------------------------------------------------------------------------------
  return: result_vector: Für jede Communication Round werden die Dauer der Berechnung, die Test-Genauigkeit und den Test-Fehler gespeichert.
  args: clients_batched: Ein Dict von Clients, wobei die Daten der Clients in Batches aufgeteilt 
        active_client_ratio: Gibt den Anteil der aktiven Clients pro Communication Round an. Anzahl der aktiven Clients wird aufgerundet.
        learning_rate: Lerning Rate für das SGD im lokalen Client Update
        comms_round: Anzahl der Communication Rounds
        E_size: Epoch Size des SGD im lokalen Client Update
  '''

  start = time.time()
  result_vector = np.zeros((comms_round,3))

  #initialize global model
  smlp_global = SimpleMLP()
  global_model = smlp_global.build(28,10)

  # gets the weights of the model with 0 entries only for initializing the control variates
  global_weights = global_model.get_weights()


  client_names = list(clients_batched.keys())


  if isinstance(learning_rate, list):
    lr = learning_rate 
  else:
    lr = learning_rate* np.array(comms_round*[1])



  #commence global training loop
  #for comm_round in range(comms_round):
  comm_round = 0
  while comm_round < comms_round:


    #get the global model' weights - will serve as the initial weight for all local models
    global_weights = global_model.get_weights()

    scaled_local_weight_list = list()

    #randomize client data - using keys
    client_names = list(clients_batched.keys())
    random.shuffle(client_names)

    active_clients = client_names[0:int(len(client_names)* active_client_ratio)]

# Ab hier beginnt das ClientUpdate()

    #loop through each active client and create new local model
    for client in active_clients:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(28,10)
        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)

        #set local model weight to the weight of the global model
        local_model.set_weights(global_weights)

        local_weights = global_weights.copy()
        for epoch_counter in range(E_size):
          for(X_client,Y_client) in clients_batched[client]:  
            
            local_grad = get_gradients(local_model,X_client, Y_client)

            # step 19 Algorithmus (von der Masterarbeit)                   
            for layer in range(len(local_grad)):
              local_weights[layer] = tf.math.add(local_weights[layer], tf.math.scalar_mul(- lr[comm_round],local_grad[layer]))
            
            local_model.set_weights(local_weights)
          
        # Die lokalen Modell-Updates werden in einer gewichteten Liste gespeichert   # step 7 Algorithmus (von der Masterarbeit)             
        scaling_factor = weight_scalling_factor(clients_batched, active_clients, client)
        scaled_weights = scale_model_weights(local_model.get_weights(),scaling_factor)
        scaled_local_weight_list.append(scaled_weights)


        #clear session to free memory after each communication round
        K.clear_session()

# Hier endet das ClientUpdate()

    # model aggregation # step 12 Algorithmus (von der Masterarbeit)   
    average_weights = sum_scaled_weights(scaled_local_weight_list)

    #update global model
    global_model.set_weights(average_weights)
  
    global_acc, global_loss = test_model(x_test, y_test, global_model, comm_round)  

    # save the duration time, test acc and test error for each communication round
    result_vector[comm_round] = [time.time()-start, global_acc, global_loss]
    
    comm_round = comm_round +1
  
  return result_vector

def SCAFFOLD(clients_batched, active_client_ratio, g_lr, l_lr, comms_round=10, E_size=1):
  '''
  Dieser Algorithmus orientiert sich an Algorithmus 1 aus:
  S. P. Karimireddy, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, A. T. Suresh. “SCAFFOLD: Stochastic Controlled Averaging for Federated Learning”. (2021) arXiv:1910.06378 v4
  --------------------------------------------------------------------------------------------------------------------------
  return: result_vector: Für jede Communication Round werden die Dauer der Berechnung, die Test-Genauigkeit und den Test-Fehler gespeichert.
  args: clients_batched: Ein Dict von Clients, wobei die Daten der Clients in Batches aufgeteilt 
        active_client_ratio: Gibt den Anteil der aktiven Clients pro Communication Round an. Anzahl der aktiven Clients wird aufgerundet.
        g_lr: Globale Lerning Rate für die Modell Aggregation
        l_lr: Locale Lerning Rate für das SGD im lokalen Client Update        
        comms_round: Anzahl der Communication Rounds
        E_size: Epoch Size des SGD im lokalen Client Update
  '''

  start = time.time()
  result_vector = np.zeros((comms_round,3))

  #initialize global model
  smlp_global = SimpleMLP()
  global_model = smlp_global.build(28,10)

  # gets the weights of the model with 0 entries only for initializing the control variates
  global_weights = global_model.get_weights()
  initial_control_variate = global_weights.copy()
  for layer in initial_control_variate:
    layer[layer!=0] = 0


  # set local control variates to 0
  clients_control_variates = dict()
  client_names = list(clients_batched.keys())
  for client_name in client_names:
    clients_control_variates[client_name] = initial_control_variate  # paper: SCAFFOLD ch. 4 

  global_control_variate = initial_control_variate # = sum([clients_control_variates[client_name] for client_name in client_names])
  clients_control_variate_plus = initial_control_variate


  if isinstance(g_lr, list):
    global_lr = g_lr 
    local_lr = l_lr
  else:
    global_lr = g_lr* np.array(comms_round*[1])
    local_lr = l_lr * np.array(comms_round*[1])


  #commence global training loop
  #for comm_round in range(comms_round):
  comm_round = 0
  while comm_round < comms_round:
    #get the global model' weights - will serve as the initial weight for all local models
    global_weights = global_model.get_weights()

    #randomize client data - using keys
    client_names = list(clients_batched.keys())
    random.shuffle(client_names)

    active_clients = client_names[0:int(len(client_names)* active_client_ratio)]

    delta_cv_list = []
    delta_model_weights_list = []

# Ab hier beginnt das ClientUpdate()

    #loop through each client and create new local model
    for client in active_clients:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(28,10)
        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)

        #set local model weight to the weight of the global model
        local_model.set_weights(global_weights)

        local_weights = global_weights.copy()
        for epoch_counter in range(E_size):
          for(X_client,Y_client) in clients_batched[client]:
            
            local_grad = get_gradients(local_model,X_client, Y_client)


            # step 10 in alg        
            for layer in range(len(local_grad)):
              local_weights[layer] = tf.math.add(local_weights[layer], tf.math.scalar_mul(- local_lr[comm_round],
                                                            tf.math.add_n([local_grad[layer],tf.math.scalar_mul(-1,clients_control_variates[client_name][layer]),
                                                                          global_control_variate[layer] ])))
              


            
            local_model.set_weights(local_weights)
          
        


        for layer in range(len(clients_control_variate_plus)):
          clients_control_variate_plus[layer] = tf.math.add_n([clients_control_variates[client_name][layer], tf.math.scalar_mul(- 1, global_control_variate[layer])  , 
                                                               tf.math.scalar_mul(1/(local_lr[comm_round]*len(clients_batched[client])),
                                                            tf.math.add(global_weights[layer],  tf.math.scalar_mul(-1,local_weights[layer] )))])
          
                      
          


        unscaled_cv = [clients_control_variate_plus[layer] - clients_control_variates[client_name][layer] for layer in range(len(clients_control_variate_plus))  ] 
        unscaled_weights = [local_weights[layer] - global_weights[layer] for layer in range(len(clients_control_variate_plus))  ]

        scaling_factor = 1/len(active_clients)  # ungewichtet Mittel
        #scaling_factor = weight_scalling_factor(clients_batched,active_clients, client) # gewichtetes Mittel

        scaled_cv = scale_model_weights(unscaled_cv,scaling_factor)
        scaled_weights = scale_model_weights(unscaled_weights,scaling_factor)


        delta_cv_list.append(scaled_cv)
        delta_model_weights_list.append(scaled_weights)

        clients_control_variates[client_name] = clients_control_variate_plus

        #clear session to free memory after each communication round
        K.clear_session()

# Hier endet das ClientUpdate()

    average_cv = sum_scaled_weights(delta_cv_list)
    average_model_weights = sum_scaled_weights(delta_model_weights_list)




    for layer in range(len(global_weights)):
              global_weights[layer] = tf.math.add(global_weights[layer], tf.math.scalar_mul(  global_lr[comm_round], average_model_weights[layer]))

    for layer in range(len(global_control_variate)):
              global_control_variate[layer] = tf.math.add(global_control_variate[layer], tf.math.scalar_mul( active_client_ratio , average_cv[layer]))


    #update global model
    global_model.set_weights(global_weights)


    global_acc, global_loss = test_model(x_test, y_test, global_model, comm_round)  

    result_vector[comm_round] = [time.time()-start, global_acc, global_loss]
    
    comm_round = comm_round +1
  
  return result_vector

def FedDyn(clients_batched, active_client_ratio, alpha, comms_round=10):
  '''
  Dieser Algorithmus orientiert sich an Algorithmus 2 aus:
  D. A. E. Acar, Y. Zhao, R. M. Navarro, M. Mattina, P. N. Whatmough, V. Saligrama. “Federated Learning Based on Dynamic Regularization”. (2021) arXiv:2111.04263 v2
  Es handelt sich hier um eine Vereinfach des normalen FedDyn-Algorithmus
  --------------------------------------------------------------------------------------------------------------------------
  return: result_vector: Für jede Communication Round werden die Dauer der Berechnung, die Test-Genauigkeit und den Test-Fehler gespeichert.
  args: clients_batched: Ein Dict von Clients, wobei die Daten der Clients in Batches aufgeteilt 
        active_client_ratio: Gibt den Anteil der aktiven Clients pro Communication Round an. Anzahl der aktiven Clients wird aufgerundet.
        alpha: Dynamischer Regulierer    
        comms_round: Anzahl der Communication Rounds
  '''


  start = time.time()
  result_vector = np.zeros((comms_round,3))

  #initialize global model
  smlp_global = SimpleMLP()
  global_model = smlp_global.build(28,10)

  # gets the weights of the model with 0 entries only for initializing the control variates
  global_weights = global_model.get_weights()
  
  initial_h = global_weights.copy()
  for layer in initial_h:
    layer[layer!=0] = 0

  clients_gradients = dict()
  for(client_name,data) in clients_batched.items():
    clients_gradients[client_name] = initial_h
  h_global = initial_h

  client_names = list(clients_batched.keys())


  #commence global training loop
  #for comm_round in range(comms_round):
  comm_round = 0
  while comm_round < comms_round:


    #get the global model' weights - will serve as the initial weight for all local models
    global_weights = global_model.get_weights()

    scaled_local_weight_list = list()
    scaled_local_weight_diff_list = list()

    #randomize client data - using keys
    client_names = list(clients_batched.keys())
    random.shuffle(client_names)

    active_clients = client_names[0:int(len(client_names)* active_client_ratio)]

    #loop through each client and create new local model
    for client in active_clients:
        smlp_local = SimpleMLP()
        local_model = smlp_local.build(28,10)
        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)

        #set local model weight to the weight of the global model
        local_model.set_weights(global_weights)

        h_local = clients_gradients[client]

        local_weights = global_weights.copy()
        diff_of_weights = global_weights.copy()

        for(X_client,Y_client) in clients_batched[client]:
          grad_glob_weights = get_gradients(local_model,X_client, Y_client)

        # Berechnung des lokalen Parameter Updates erfolgt wie in Algorithmus in Federated Learning Based on Dynamic Regularization
        for layer in range(len(grad_glob_weights)):
          local_weights[layer] = tf.math.add(global_weights[layer], tf.math.scalar_mul(- 1/alpha,  tf.math.add(grad_glob_weights[layer], tf.math.scalar_mul(- 1, h_local[layer]) ) ))
            
        for layer in range(len(h_local)):
          h_local[layer] = tf.math.add(h_local[layer], tf.math.scalar_mul(- alpha,  tf.math.add(local_weights[layer], tf.math.scalar_mul(- 1, global_weights[layer]) ) ) )

        for layer in range(len(diff_of_weights)):
          diff_of_weights[layer] = tf.math.add( local_weights[layer] , tf.math.scalar_mul(- 1, global_weights[layer] ))
  
                          
        scaling_factor = weight_scalling_factor(clients_batched, active_clients, client) # gewichtet Mittel
        #scaling_factor = 1 / len(active_clients) # ungewichtet Mittel


        scaled_weights = scale_model_weights(local_weights,scaling_factor)
        scaled_local_weight_list.append(scaled_weights)

        #scaling_factor = 1 / len(client_names) # ungewichtet Mittel

        scaled_weights_diff = scale_model_weights(diff_of_weights,scaling_factor)
        scaled_local_weight_diff_list.append(scaled_weights_diff)



        clients_gradients[client] = h_local


        #clear session to free memory after each communication round
        K.clear_session()



    average_weights_diff = sum_scaled_weights(scaled_local_weight_diff_list)

    for layer in range(len(h_global)):
          h_global[layer] = tf.math.add( h_global[layer] , tf.math.scalar_mul(- alpha/ len(client_names), average_weights_diff[layer] ))

    average_weights = sum_scaled_weights(scaled_local_weight_list)

    for layer in range(len(h_global)):
        average_weights[layer] = tf.math.add( average_weights[layer] , tf.math.scalar_mul(-1/ alpha, h_global[layer] ))


    #update global model
    global_model.set_weights(average_weights)
  
    global_acc, global_loss = test_model(x_test, y_test, global_model, comm_round)  

    result_vector[comm_round] = [time.time()-start, global_acc, global_loss]
    
    comm_round = comm_round +1
  
  return result_vector

"""# fixierte Hyperparameter"""

B_size = 250  #Bach size
E_size = 1    # number of Epochs
comms_round = 30 # Anzahl an Iterationen

dirichlet_param = 0.3 # Der Parameter für die Dirichlet Verteilung

number_of_clients = 50 # Anzahl der Clients
active_client_ratio = 0.2 # 0.8 #ratio of active clients per iteration


learning_rate = 0.3 # Learning Rate für FedAvg
learning_rate_glob = 1.0 # globale Learning Rate für SCAFFOLD
learning_rate_loc = 0.3 # lokale Learning Rate für SCAFFOLD
alpha = 10 # Parameter für FedDyn

loss = tf.keras.losses.SparseCategoricalCrossentropy() 
metrics = ['accuracy',tf.keras.metrics.SparseCategoricalAccuracy()]
optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)

"""## Test 0: FedAvg: acc vs err"""

B_size = 150  #Bach size
E_size = 1    # number of Epochs
comms_round = 50 # Anzahl an Iterationen

dirichlet_param = 0.3 # Der Parameter für die Dirichlet Verteilung

number_of_clients = 50 # Anzahl der Clients
active_client_ratio = 0.2 # 0.8 #ratio of active clients per iteration


learning_rate = 0.3 # Learning Rate für FedAvg
learning_rate_glob = 1.0 # globale Learning Rate für SCAFFOLD
learning_rate_loc = 0.3 # lokale Learning Rate für SCAFFOLD
alpha = 10 # Parameter für FedDyn

loss = tf.keras.losses.SparseCategoricalCrossentropy() 
metrics = ['accuracy',tf.keras.metrics.SparseCategoricalAccuracy()]
optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)

# TEST for non-iid data


fedavg_test_0 = dict()

list_fedavg =  ['iid', 0.6, 0.3, 0.01]


for param in list_fedavg:

  if param == 'iid':
    print('('+param +')')
    clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients)
  else:
    print('('+str(param) +')')
    clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients, dirichlet_param=param)
  clients_batched = get_clients_batched(clients, B_size)

  fedavg_test_0[param] = FedAvg(clients_batched,active_client_ratio, 0.5, comms_round)

plt.figure(figsize=(10, 5))

for item in fedavg_test_0.keys():
  if item == 'iid':
    plt.plot(np.arange(comms_round)+1, fedavg_test_0[item][:,1], label= 'FedAvg iid')
  elif item==3:
    print('ok')
  else:
    plt.plot(np.arange(comms_round)+1, fedavg_test_0[item][:,1], label= 'FedAvg Dirichlet(' +str(item) + ')')



plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(2.36, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('FedAvg', fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot1.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

plt.figure(figsize=(10, 5))

for item in fedavg_test_0.keys():
  if item == 'iid':
    plt.plot(np.arange(comms_round)+1, fedavg_test_0[item][:,2], label= 'FedAvg iid')
  
  elif item==3:
    print('ok')
  else:
    plt.plot(np.arange(comms_round)+1, fedavg_test_0[item][:,2], label= 'FedAvg Dirichlet(' +str(item) + ')')



plt.ylabel('Test Error', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(2.36, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('FedAvg', fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot2.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 0: SCAFFOLD: acc vs err"""

B_size = 150  #Bach size
E_size = 1    # number of Epochs
comms_round = 50 # Anzahl an Iterationen

dirichlet_param = 0.3 # Der Parameter für die Dirichlet Verteilung

number_of_clients = 50 # Anzahl der Clients
active_client_ratio = 0.2 # 0.8 #ratio of active clients per iteration


learning_rate = 0.3 # Learning Rate für FedAvg
learning_rate_glob = 1.0 # globale Learning Rate für SCAFFOLD
learning_rate_loc = 0.3 # lokale Learning Rate für SCAFFOLD
alpha = 10 # Parameter für FedDyn

loss = tf.keras.losses.SparseCategoricalCrossentropy() 
metrics = ['accuracy',tf.keras.metrics.SparseCategoricalAccuracy()]
optimizer =  tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate)

# TEST for non-iid data


scaffold_test_0 = dict()

list_scaffold =  ['iid', 0.6, 0.3, 0.01]


for param in list_scaffold:

  if param == 'iid':
    print('('+param +')')
    clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients)
  else:
    print('('+str(param) +')')
    clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients, dirichlet_param=param)
  clients_batched = get_clients_batched(clients, B_size)

  #scaffold_test_0[param] = FedAvg(clients_batched,active_client_ratio, 0.5, comms_round) 
  scaffold_test_0[param] = SCAFFOLD(clients_batched,active_client_ratio, 1, 0.3, comms_round)

plt.figure(figsize=(10, 5))

for item in scaffold_test_0.keys():
  if item == 'iid':
    plt.plot(np.arange(comms_round)+1, scaffold_test_0[item][:,1], label= 'SCAFFOLD iid')
  elif item==3:
    print('ok')
  else:
    plt.plot(np.arange(comms_round)+1, scaffold_test_0[item][:,1], label= 'SCAFFOLD Dirichlet(' +str(item) + ')')



plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(2.36, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('SCAFFOLD', fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot3.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

plt.figure(figsize=(10, 5))

for item in scaffold_test_0.keys():
  if item == 'iid':
    plt.plot(np.arange(comms_round)+1, scaffold_test_0[item][:,2], label= 'SCAFFOLD iid')
  elif item==3:
    print('ok')
  else:
    plt.plot(np.arange(comms_round)+1, scaffold_test_0[item][:,2], label= 'SCAFFOLD Dirichlet(' +str(item) + ')')



plt.ylabel('Test Error', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(2.36, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('SCAFFOLD', fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot4.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 1: Learning Rate FedAvg"""

# TEST for iid data
test='iid'

fedavg_test1 = dict()
list_fedavg = [0.1, 0.3, 0.5, 0.75, 1]


clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients)
clients_batched = get_clients_batched(clients, B_size)


for param in list_fedavg:
  print('('+str(param) +')')
  fedavg_test1[param] = FedAvg(clients_batched,active_client_ratio, param, comms_round)

# TEST for non-iid data
test='non-iid'

fedavg_test1 = dict()
list_fedavg =  [0.1, 0.3, 0.5, 0.75, 1]


clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients, dirichlet_param=dirichlet_param)
clients_batched = get_clients_batched(clients, B_size)


for param in list_fedavg:
  print('('+str(param) +')')
  fedavg_test1[param] = FedAvg(clients_batched,active_client_ratio, param, comms_round)

plt.figure(figsize=(10, 5))

for item in fedavg_test1.keys():
  plt.plot(np.arange(comms_round)+1, fedavg_test1[item][:,1], label='FedAvg: lr='+ str(item))



plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(2.36, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('FedAvg - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 1: Learning Rate SCAFFOLD"""

# TEST for iid data
test='iid'

scaffold_test1 = dict()
list_scaffold_l = [0.1,0.3, 0.5, 0.75, 1] 
list_scaffold_g = [1.0] #[0.1,0.3,0.5, 0.75,1]  

clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients)
clients_batched = get_clients_batched(clients, B_size)

for param1 in list_scaffold_g:
  for param2 in list_scaffold_l:
    print('('+str(param1) +", "+ str(param2) +')')
    scaffold_test1['('+str(param1) +", "+ str(param2) +')'] = SCAFFOLD(clients_batched,active_client_ratio, param1, param2, comms_round)

# TEST for non-iid data
test='non-iid'

scaffold_test1 = dict()
list_scaffold_l = [0.1,0.3, 0.5, 0.75, 1] #[0.1,0.3, 0.5, 0.75, 1] #[0.35] #[0.05,0.2,0.35, 0.5, 0.65, 0.8, 0.95]
list_scaffold_g = [1.0] #[0.1,0.3,0.5, 0.75,1]  

clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients, dirichlet_param=dirichlet_param)
clients_batched = get_clients_batched(clients, B_size)

for param1 in list_scaffold_g:
  for param2 in list_scaffold_l:
    print('('+str(param1) +", "+ str(param2) +')')
    scaffold_test1['('+str(param1) +", "+ str(param2) +')'] = SCAFFOLD(clients_batched,active_client_ratio, param1, param2, comms_round)

plt.figure(figsize=(10, 5))

for item in scaffold_test1.keys():
  plt.plot(np.arange(comms_round)+1, scaffold_test1[item][:,1], label='SCAFFOLD: (g_lr, l_lr) ='+ str(item) )

plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.65, -0.02))
plt.grid()
#plt.ylim(0.59, 0.83)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('SCAFFOLD - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 2: Batch Size FedAvg"""

# TEST for iid data
test='iid'

fedavg_test2 = dict()
list_fedavg =  [10, 50, 100, 500 , 1000]  

clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients)


for param in list_fedavg:
    print('('+str(param) +')')
    clients_batched = get_clients_batched(clients, param)
    fedavg_test2[param] = FedAvg(clients_batched,active_client_ratio,learning_rate, comms_round)

# TEST for non-iid data
test='non-iid'

fedavg_test2 = dict()
list_fedavg =  [10, 50, 100, 500 , 1000]  


clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients, dirichlet_param=dirichlet_param)

for param in list_fedavg:
    print('('+str(param) +')')
    clients_batched = get_clients_batched(clients, param)
    fedavg_test2[param] = FedAvg(clients_batched,active_client_ratio,learning_rate, comms_round)

plt.figure(figsize=(10, 5))


for item in fedavg_test2.keys():
  plt.plot(np.arange(comms_round)+1, fedavg_test2[item][:,1], label='FedAvg: batch_size='+ str(item))
  #plt.plot(fedavg_test2[item][:,0], fedavg_test2[item][:,1], label='FedAvg: batch_size='+ str(item))
plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.52, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('FedAvg - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 2: Batch Size SCAFFOLD"""

# TEST for iid data
test='iid'

scaffold_test2 = dict()
list_scaffold = [10, 50, 100, 500 , 1000]  

clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients)


for param in list_scaffold:
    print('('+str(param) +')')
    clients_batched = get_clients_batched(clients, param)
    scaffold_test2[str(param) ] = SCAFFOLD(clients_batched,active_client_ratio, learning_rate_glob, learning_rate_loc, comms_round)

# TEST for non-iid data
test='non-iid'

scaffold_test2 = dict()
list_scaffold = [10, 50, 100, 500 , 1000]  

clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients, dirichlet_param=dirichlet_param)

for param in list_scaffold:
    print('('+str(param) +')')
    clients_batched = get_clients_batched(clients, param)
    scaffold_test2[str(param) ] = SCAFFOLD(clients_batched,active_client_ratio, learning_rate_glob, learning_rate_loc, comms_round)

plt.figure(figsize=(10, 5))

for item in scaffold_test2.keys():
  plt.plot(np.arange(comms_round)+1, scaffold_test2[item][:,1], label='SCAFFOLD: batch_size='+ str(item) )

plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.65, -0.02))
plt.grid()
#plt.ylim(0.8, 0.93)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('SCAFFOLD - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 3: Epoch Size FedAvg"""

# TEST for iid data
test='iid'

fedavg_test3 = dict()
list_fedavg = [1, 2, 3]  

clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients)
clients_batched = get_clients_batched(clients, B_size)

for param in list_fedavg:
    print('('+str(param) +')')
    E_size = param
    fedavg_test3[param] = FedAvg(clients_batched,active_client_ratio, learning_rate, comms_round, param)

# TEST for non-iid data
test='non-iid'

fedavg_test3 = dict()
list_fedavg = [1, 2, 3]  

clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients, dirichlet_param=dirichlet_param)
clients_batched = get_clients_batched(clients, B_size)

for param in list_fedavg:
    print('('+str(param) +')')
    fedavg_test3[param] = FedAvg(clients_batched,active_client_ratio, learning_rate, comms_round,param)

plt.figure(figsize=(10, 5))

for item in fedavg_test3.keys():
  plt.plot(np.arange(comms_round)+1, fedavg_test3[item][:,1], label='FedAvg: epoch_size='+ str(item) )
  #plt.plot(fedavg_test3[item][:,0], fedavg_test3[item][:,1], label='FedAvg: epoch_size='+ str(item) )

plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.52, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('FedAvg - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 4: Epoch Size SCAFFOLD"""

# TEST for iid data
test='iid'

scaffold_test3 = dict()
list_scaffold = [1, 2, 3]  

clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients)
clients_batched = get_clients_batched(clients, B_size)

for param in list_scaffold:
    print('('+str(param) +')')
    scaffold_test3[str(param) ] = SCAFFOLD(clients_batched,active_client_ratio, learning_rate_glob, learning_rate_loc, comms_round,param)

# TEST for non-iid data
test='non-iid'

scaffold_test3 = dict()
list_scaffold = [1, 2, 3]  

clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients, dirichlet_param=dirichlet_param)
clients_batched = get_clients_batched(clients, B_size)

for param in list_scaffold:
    print('('+str(param) +')')
    E_size = param
    scaffold_test3[str(param) ] = SCAFFOLD(clients_batched,active_client_ratio, learning_rate_glob, learning_rate_loc, comms_round, param)

plt.figure(figsize=(10, 5))


for item in scaffold_test3.keys():
  plt.plot(np.arange(comms_round)+1, scaffold_test3[item][:,1], label='SCAFFOLD: epoch_size='+ str(item) )
  #plt.plot(scaffold_test3[item][:,0], scaffold_test3[item][:,1], label='SCAFFOLD: epoch_size='+ str(item) )

plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.65, -0.02))
plt.grid()
#plt.ylim(0.8, 0.93)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('SCAFFOLD - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 5: Client Size FedAvg"""

# TEST for iid data
test='iid'

fedavg_test4 = dict()
list_fedavg = [10, 25, 50, 100, 200]  

for param in list_fedavg:
    print('('+str(param) +')')
    clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=param)
    clients_batched = get_clients_batched(clients, B_size)
    fedavg_test4[param] = FedAvg(clients_batched,active_client_ratio,learning_rate, comms_round)

# TEST for non-iid data
test='non-iid'

fedavg_test4 = dict()
list_fedavg = [10, 25, 50, 100, 200]  

for param in list_fedavg:
    print('('+str(param) +')')

    clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=param,  dirichlet_param=dirichlet_param)
    clients_batched = get_clients_batched(clients, B_size)
    fedavg_test4[param] = FedAvg(clients_batched,active_client_ratio, learning_rate, comms_round)

plt.figure(figsize=(10, 5))


for item in fedavg_test4.keys():
  plt.plot(np.arange(comms_round)+1, fedavg_test4[item][:,1], label='FedAvg: Clients='+ str(item) )


plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.5, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('FedAvg - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 5: Client Size SCAFFOLD"""

# TEST for iid data
test='iid'


scaffold_test4 = dict()
list_scaffold = [10, 25, 50, 100, 200]  

for param in list_scaffold:
    print('('+str(param) +')')
    clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=param)
    clients_batched = get_clients_batched(clients, B_size)
    scaffold_test4[str(param) ] = SCAFFOLD(clients_batched,active_client_ratio, learning_rate_glob, learning_rate_loc, comms_round)

# TEST for non-iid data
test='non-iid'


scaffold_test4 = dict()
list_scaffold = [10, 25, 50, 100, 200]  

for param in list_scaffold:
    print('('+str(param) +')')
    clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=param, dirichlet_param=dirichlet_param)
    clients_batched = get_clients_batched(clients, B_size)
    scaffold_test4[str(param) ] = SCAFFOLD(clients_batched,active_client_ratio, learning_rate_glob, learning_rate_loc, comms_round)

plt.figure(figsize=(10, 5))


for item in scaffold_test4.keys():
  plt.plot(np.arange(comms_round)+1, scaffold_test4[item][:,1], label='SCAFFOLD: Clients='+ str(item) )

plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.48, -0.02))
plt.grid()
#plt.ylim(0.8, 0.93)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('SCAFFOLD - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 6: Active Client Size FedAvg"""

# TEST for iid data
test='iid'

number_of_clients2 = 100

fedavg_test5 = dict()
list_fedavg = [1, 5, 25, 50, 100]  

clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients2 )

for param in list_fedavg:
    print('('+str(param) +')')
    
    clients_batched = get_clients_batched(clients, B_size)
    fedavg_test5[param] = FedAvg(clients_batched,param/100,learning_rate, comms_round)

# TEST for non-iid data
test='non-iid'

number_of_clients2 = 100

fedavg_test5 = dict()
list_fedavg = [1, 5, 25, 50, 100]  

clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients2,  dirichlet_param=dirichlet_param)

for param in list_fedavg:
    print('('+str(param) +')')
    
    clients_batched = get_clients_batched(clients, B_size)
    fedavg_test5[param] = FedAvg(clients_batched,param/100, 0.3, comms_round)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))

for item in fedavg_test5.keys():
  plt.plot(np.arange(comms_round)+1, fedavg_test5[item][:,1], label='FedAvg: Aktive Clients='+ str(item)) 

plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.55, -0.02))
plt.grid()
#plt.ylim(0.80, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('FedAvg - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test 6: Active Client Size SCAFFOLD"""

# TEST for iid data
test='iid'

number_of_clients2= 100

scaffold_test5  = dict()
list_scaffold = [1, 5, 25, 50, 100]  

clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients2)


for param in list_scaffold:
    print('('+str(param) +')')
    
    clients_batched = get_clients_batched(clients, B_size)
    scaffold_test5[str(param) ] = SCAFFOLD(clients_batched,param/100, learning_rate_glob, learning_rate_loc, comms_round)

# TEST for non-iid data
test='non-iid'

number_of_clients2= 100

scaffold_test5 = dict()
list_scaffold = [1, 5, 25, 50, 100]  # Gebe Anzahl der aktiven Clients 

clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients2, dirichlet_param=dirichlet_param)

for param in list_scaffold:
    print('('+str(param) +')')
    
    clients_batched = get_clients_batched(clients, B_size)
    scaffold_test5[str(param) ] = SCAFFOLD(clients_batched,param/100, learning_rate_glob, learning_rate_loc, comms_round)

plt.figure(figsize=(10, 5))

for item in scaffold_test5.keys():
  plt.plot(np.arange(comms_round)+1, scaffold_test5[item][:,1], label='SCAFFOLD: Aktive Clients='+ item) 

plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.59, -0.02))
plt.grid()
#plt.ylim(0.8, 0.93)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('SCAFFOLD - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()

"""## Test: Learning Rate FedDyn"""

# TEST for iid data
test='iid'

feddyn_test1 = dict()
list_feddyn = [ 10, 20, 25,30, 40]


clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients)
clients_batched = get_clients_unbatched(clients)


for param in list_feddyn:
  print('('+str(param) +')')
  feddyn_test1[param] = FedDyn(clients_batched,active_client_ratio, param, comms_round)

# TEST for non-iid data
test='non-iid'

feddyn_test1 = dict()
list_feddyn =  [ 10, 20, 25,30, 40]


clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=number_of_clients,  dirichlet_param=dirichlet_param)
clients_batched = get_clients_unbatched(clients)


for param in list_feddyn:
  print('('+str(param) +')')
  feddyn_test1[param] = FedDyn(clients_batched,active_client_ratio, param, comms_round)

plt.figure(figsize=(10, 5))

for item in feddyn_test1.keys():
  plt.plot(np.arange(comms_round)+1, feddyn_test1[item][:,1], label='FedDyn: alpha='+ str(item))

plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.43, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('FedDyn - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()



"""## Test: Client Size FedDyn"""

# TEST for iid data
test='iid'

feddyn_test2 = dict()
list_feddyn = [10, 25, 50, 100, 200]  

for param in list_feddyn:
    print('('+str(param) +')')
    clients = create_clients_iid(x_train_sorted,y_train_sorted, num_clients=param)
    clients_batched = get_clients_unbatched(clients)
    feddyn_test2[param] = FedDyn(clients_batched,active_client_ratio, alpha, comms_round)

# TEST for non-iid data
test='non-iid'

feddyn_test2 = dict()
list_feddyn =  [10, 25, 50, 100, 200]  

for param in list_feddyn:
    print('('+str(param) +')')

    clients =  create_clients_non_iid(x_train_sorted,y_train_sorted, num_clients=param, dirichlet_param=dirichlet_param)
    clients_batched = get_clients_unbatched(clients)
    feddyn_test2[param] = FedDyn(clients_batched,active_client_ratio, alpha, comms_round)

plt.figure(figsize=(10, 5))

for item in feddyn_test2.keys():
  plt.plot(np.arange(comms_round)+1, feddyn_test2[item][:,1], label='FedDyn: Clients='+ str(item) )

plt.ylabel('Test Accuracy', fontsize=16)
plt.xlabel('Communication Rounds', fontsize=16)
plt.legend(fontsize=16, loc='lower right', bbox_to_anchor=(1.5, -0.02))
plt.grid()
#plt.ylim(0.87, 0.95)
plt.xlim(0, comms_round) 
#plt.xlim(15, 20)
plt.title('FedDyn - '+test, fontsize=16)
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.savefig('plot.pdf', dpi=1000, bbox_inches='tight')
# plt.show()